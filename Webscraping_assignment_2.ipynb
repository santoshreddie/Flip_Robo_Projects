{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location, company_name,\n",
    "experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore”\n",
    "   in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reponse is : <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Role_name</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Informatica MDM</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assistant Vice President - MIS &amp; Reporting ( B...</td>\n",
       "      <td>INTERTRUSTVITEOS CORPORATE AND FUND SERVICES P...</td>\n",
       "      <td>Mumbai, Bangalore/Bengaluru</td>\n",
       "      <td>12-18 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>SA Tech Software (I) Pvt. Ltd.</td>\n",
       "      <td>Kolkata, Pune, Chennai, Bangalore/Bengaluru, D...</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data analyst - Google Analytics</td>\n",
       "      <td>H and M Hennes and Mauritz (P) Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior/Regular Business Analyst / Data Analyst</td>\n",
       "      <td>Luxoft</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Role_name  \\\n",
       "0    Data Scientist / Data Analyst -Business Analyst   \n",
       "1                     Data Analyst - Informatica MDM   \n",
       "2  Assistant Vice President - MIS & Reporting ( B...   \n",
       "3                                       Data Analyst   \n",
       "4                                       Data Analyst   \n",
       "5                                       Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                    Data analyst - Google Analytics   \n",
       "9     Senior/Regular Business Analyst / Data Analyst   \n",
       "\n",
       "                                        Company_name  \\\n",
       "0                 Inflexion Analytix Private Limited   \n",
       "1                Shell India Markets Private Limited   \n",
       "2  INTERTRUSTVITEOS CORPORATE AND FUND SERVICES P...   \n",
       "3                           Myntra Designs Pvt. Ltd.   \n",
       "4                           Myntra Designs Pvt. Ltd.   \n",
       "5                           Myntra Designs Pvt. Ltd.   \n",
       "6                           Myntra Designs Pvt. Ltd.   \n",
       "7                     SA Tech Software (I) Pvt. Ltd.   \n",
       "8                H and M Hennes and Mauritz (P) Ltd.   \n",
       "9                                             Luxoft   \n",
       "\n",
       "                                            Location Experience  \n",
       "0  Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...    0-3 Yrs  \n",
       "1                                Bangalore/Bengaluru    6-9 Yrs  \n",
       "2                        Mumbai, Bangalore/Bengaluru  12-18 Yrs  \n",
       "3                                Bangalore/Bengaluru    3-6 Yrs  \n",
       "4                                Bangalore/Bengaluru    3-6 Yrs  \n",
       "5                                Bangalore/Bengaluru    4-9 Yrs  \n",
       "6                                Bangalore/Bengaluru    4-8 Yrs  \n",
       "7  Kolkata, Pune, Chennai, Bangalore/Bengaluru, D...    1-3 Yrs  \n",
       "8                                Bangalore/Bengaluru    4-7 Yrs  \n",
       "9                                Bangalore/Bengaluru    3-6 Yrs  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def naukarijob(url):\n",
    "    #at first lets check the requests of the site, if we get responce of 200 then we have to scrap\n",
    "    request = requests.get(url)\n",
    "    print(\"The reponse is :\", request)\n",
    "\n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #now we have to search the job description and location to search the data\n",
    "    #there are two search boxes at the starting page\n",
    "    jobdescription = driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Analyst\")\n",
    "\n",
    "    #i have used find element and searched the element by id and used send_keys method for entering text into the search\n",
    "\n",
    "    locationinfo = driver.find_element_by_id(\"qsb-location-sugg\").send_keys(\"Bangalore\")\n",
    "\n",
    "    #here i am searching for jobsin bangalore location\n",
    "    #and at last i am going to click on search button\n",
    "\n",
    "    button = driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "    button.click()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    #now lets scrap some detais by like job location, experience and others \n",
    "    time.sleep(3)\n",
    "    \n",
    "    #creating lists for storing the results\n",
    "    roles_list = []\n",
    "    comapny_list = []\n",
    "    location_list = []\n",
    "    exp_list = []\n",
    "    \n",
    "    #here lets start with the titles block here i am using xpath to find webelemunts to fetch job titles pf that page\n",
    "    #as we can see i have using slicling methd in the for loop to fetch only 10 values \n",
    "    # i have used exception handling so that we can execute the code easily with out any errors or exceptions\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        titles = driver.find_elements_by_xpath(\"//a[@class = 'title fw500 ellipsis']\")\n",
    "        for title in titles[:10]:\n",
    "            roles_list.append(title.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    # this is the blockfor comapny details, here  also i have used xpath for fetching\n",
    "\n",
    "    try:\n",
    "        company = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "        for com in company[:10]:\n",
    "            comapny_list.append(com.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    #this is the block for location data\n",
    "\n",
    "    try:\n",
    "        location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "        for loc in location[:10]:\n",
    "            location_list.append(loc.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #this is the block for experience data \n",
    "\n",
    "    try:\n",
    "        experience = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "        for exp in experience[:10]:\n",
    "            exp_list.append(exp.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    #now letscreatea dataframeand store those results in that dataframe so that we can use them for later use\n",
    "    \n",
    "    naukari_job1=pd.DataFrame({})\n",
    "         \n",
    "    naukari_job1['Role_name']=roles_list\n",
    "\n",
    "    naukari_job1['Company_name']=comapny_list\n",
    "\n",
    "    naukari_job1['Location']=location_list\n",
    "    \n",
    "    naukari_job1['Experience']=exp_list\n",
    "    \n",
    "    return naukari_job1\n",
    "\n",
    "    #saving it to csv file\n",
    "    \n",
    "    naukari_job1.to_csv('naukari_job1.csv')\n",
    "    \n",
    "\n",
    "#calling the function    \n",
    "naukarijob(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Scientist” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location,\n",
    "company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter\n",
    "“Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done\n",
    "manually.\n",
    "2.Please note that you have to scrape full job description. For that you may have to\n",
    "open each job separately as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reponse is : <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Comapny</th>\n",
       "      <th>Location</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...</td>\n",
       "      <td>Job description\\nJob Role : Data Scientist/Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist, Modeling</td>\n",
       "      <td>Nielsen</td>\n",
       "      <td>Kolkata, Gurgaon/Gurugram, Bangalore/Bengaluru...</td>\n",
       "      <td>Job description\\nWe wont say we can predict th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Kochi/Cochin, Indore, Hyderabad/Secunderabad, ...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specialist I - Data Scientist</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nResponsibilities and Key Resu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Intel Technology India Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\n\\n\\nWe are seeking an outstan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SDE Lead Data Scientist-L3</td>\n",
       "      <td>Huawei Technologies India Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nBusiness &amp; Team overview:\\nFo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Computational Design Lead Data Scientist-L3</td>\n",
       "      <td>Huawei Technologies India Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nBusiness &amp; Team overview:\\nFo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiring For DATA Scientist - ON Contract Basis ...</td>\n",
       "      <td>GlobalEdx Learning and Technology Solution Pvt...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, M...</td>\n",
       "      <td>Job description\\nDear Aspirant,\\n\\nGreetings f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0    Data Scientist / Data Analyst -Business Analyst   \n",
       "1                    Senior Data Scientist, Modeling   \n",
       "2                                                  -   \n",
       "3                      Specialist I - Data Scientist   \n",
       "4                                Lead Data Scientist   \n",
       "5                                                  -   \n",
       "6                                                  -   \n",
       "7                         SDE Lead Data Scientist-L3   \n",
       "8        Computational Design Lead Data Scientist-L3   \n",
       "9  Hiring For DATA Scientist - ON Contract Basis ...   \n",
       "\n",
       "                                             Comapny  \\\n",
       "0                 Inflexion Analytix Private Limited   \n",
       "1                                            Nielsen   \n",
       "2                                                  -   \n",
       "3                              Philips India Limited   \n",
       "4                     Intel Technology India Pvt Ltd   \n",
       "5                                                  -   \n",
       "6                                                  -   \n",
       "7                  Huawei Technologies India Pvt Ltd   \n",
       "8                  Huawei Technologies India Pvt Ltd   \n",
       "9  GlobalEdx Learning and Technology Solution Pvt...   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...   \n",
       "1  Kolkata, Gurgaon/Gurugram, Bangalore/Bengaluru...   \n",
       "2  Kochi/Cochin, Indore, Hyderabad/Secunderabad, ...   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, M...   \n",
       "\n",
       "                                         Description  \n",
       "0  Job description\\nJob Role : Data Scientist/Dat...  \n",
       "1  Job description\\nWe wont say we can predict th...  \n",
       "2                                                  -  \n",
       "3  Job description\\nResponsibilities and Key Resu...  \n",
       "4  Job description\\n\\n\\nWe are seeking an outstan...  \n",
       "5                                                  -  \n",
       "6                                                  -  \n",
       "7  Job description\\nBusiness & Team overview:\\nFo...  \n",
       "8  Job description\\nBusiness & Team overview:\\nFo...  \n",
       "9  Job description\\nDear Aspirant,\\n\\nGreetings f...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def naukarijob2(url):\n",
    "    \n",
    "    #Every time before scraping we have to check the responce from website if it is 200 we can scrap any data\n",
    "    request = requests.get(url)\n",
    "    print(\"The reponse is :\", request)\n",
    "\n",
    "    #lets get the the chrome driver file stored in system so that we can acess the browser\n",
    "\n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    #now we have to search the job description and location to search the data\n",
    "    #there are two search boxes at the starting page\n",
    "    jobdescription = driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Scientist\")\n",
    "\n",
    "    #i have used find element and searched the element by id and used send_keys method for entering text into the search\n",
    "\n",
    "    locationinfo = driver.find_element_by_id(\"qsb-location-sugg\").send_keys(\"Bangalore\")\n",
    "\n",
    "    #here i am searching for jobsin bangalore location\n",
    "    #and at last i am going to click on search button\n",
    "\n",
    "    button = driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "    button.click()\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    #now we have to scrap details like jobtitle, description , location etc\n",
    "    \n",
    "    #creating lists for storing results \n",
    "    job_title = []\n",
    "    job_location = []\n",
    "    company_name = []\n",
    "    description = []\n",
    "    \n",
    "    #getting location data\n",
    "    \n",
    "    try:\n",
    "        location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "        for loc in location[:10]:\n",
    "            job_location.append(loc.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    #getting hyperlinks of each job so that we can can iterate to each link and can collect the data\n",
    "\n",
    "\n",
    "    urls = []\n",
    "    for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "        urls.append(i.get_attribute('href'))\n",
    "        \n",
    "    \n",
    "    #setting for loop so that we can loop through each of the link and collets the data\n",
    "    \n",
    "    for ii in urls[:10]:\n",
    "        driver.get(ii)\n",
    "        time.sleep(3)\n",
    "        #fetching job_titles\n",
    "        try:\n",
    "            job = driver.find_element_by_xpath(\"//h1[@class='jd-header-title']\")\n",
    "            job_title.append(job.text)\n",
    "        except NoSuchElementException:\n",
    "            job_title.append(\"-\")\n",
    "        \n",
    "    #fetching description\n",
    "        \n",
    "        try:\n",
    "            job_desc = driver.find_element_by_xpath(\"//section[@class='job-desc']\")\n",
    "            description.append(job_desc.text)\n",
    "        except NoSuchElementException:\n",
    "            description.append(\"-\")\n",
    "        \n",
    "    #fetching company name\n",
    "             \n",
    "        try:\n",
    "            company = driver.find_element_by_xpath(\"//div[@class='jd-header-comp-name']/a\")\n",
    "            company_name.append(company.text)\n",
    "        except NoSuchElementException:\n",
    "            company_name.append(\"-\")\n",
    "            \n",
    "    #lets create a dataframe to store resullts  \n",
    "    \n",
    "    naukari_jobs2 = pd.DataFrame({})\n",
    "    \n",
    "    naukari_jobs2[\"Title\"] = job_title\n",
    "    \n",
    "    naukari_jobs2[\"Comapny\"]= company_name\n",
    "    \n",
    "    naukari_jobs2[\"Location\"] = job_location\n",
    "    \n",
    "    naukari_jobs2[\"Description\"]  = description\n",
    "    \n",
    "\n",
    "    return naukari_jobs2\n",
    "\n",
    "    #saving it to csv file for later use\n",
    "    naukari_jobs2.to_csv(\"naukari_jobs2.csv\")\n",
    "\n",
    "#calling function         \n",
    "naukarijob2(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the\n",
    "webpage as shown below:\n",
    "    You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reponse is : <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst- Data Scientist</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - High growth VC backed Influen...</td>\n",
       "      <td>Ravgins International Pvt. Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Bangalore/Benga...</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mobikwik</td>\n",
       "      <td>New Delhi, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DATA Scientist – Gurgaon (Exp 3-6 years)</td>\n",
       "      <td>CRESCENDO GLOBAL LEADERSHIP HIRING INDIA PRIVA...</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DATA Scientist – Gurgaon (Exp 3-6 years)</td>\n",
       "      <td>CRESCENDO GLOBAL LEADERSHIP HIRING INDIA PRIVA...</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Noida</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>Noida</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Blitz Jobs</td>\n",
       "      <td>Noida</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengal...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0    Data Scientist / Data Analyst -Business Analyst   \n",
       "1                   Business Analyst- Data Scientist   \n",
       "2  Data Scientist - High growth VC backed Influen...   \n",
       "3                                     Data Scientist   \n",
       "4                                     Data Scientist   \n",
       "5           DATA Scientist – Gurgaon (Exp 3-6 years)   \n",
       "6           DATA Scientist – Gurgaon (Exp 3-6 years)   \n",
       "7                             Data Scientist - Noida   \n",
       "8                                     Data Scientist   \n",
       "9         Data Scientist - Python & Machine Learning   \n",
       "\n",
       "                                             Company  \\\n",
       "0                 Inflexion Analytix Private Limited   \n",
       "1                                              Wipro   \n",
       "2                    Ravgins International Pvt. Ltd.   \n",
       "3                             IBM India Pvt. Limited   \n",
       "4                                           Mobikwik   \n",
       "5  CRESCENDO GLOBAL LEADERSHIP HIRING INDIA PRIVA...   \n",
       "6  CRESCENDO GLOBAL LEADERSHIP HIRING INDIA PRIVA...   \n",
       "7     Optum Global Solutions (India) Private Limited   \n",
       "8                                         Blitz Jobs   \n",
       "9                                FUTURES AND CAREERS   \n",
       "\n",
       "                                            Location Experience  \n",
       "0  Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...    0-3 Yrs  \n",
       "1                            Noida, Gurgaon/Gurugram    2-5 Yrs  \n",
       "2  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...    3-5 Yrs  \n",
       "3  Noida, Hyderabad/Secunderabad, Bangalore/Benga...    4-9 Yrs  \n",
       "4           New Delhi, Gurgaon/Gurugram, Delhi / NCR    3-5 Yrs  \n",
       "5                      Gurgaon/Gurugram, Delhi / NCR    3-6 Yrs  \n",
       "6                      Gurgaon/Gurugram, Delhi / NCR    3-6 Yrs  \n",
       "7                                              Noida    3-5 Yrs  \n",
       "8                                              Noida    3-5 Yrs  \n",
       "9  Hyderabad/Secunderabad, Pune, Bangalore/Bengal...    2-7 Yrs  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def job3(url):\n",
    "\n",
    "    #Every time before scraping we have to check the responce from website if it is 200 we can scrap any data\n",
    "    request = requests.get(url)\n",
    "    print(\"The reponse is :\", request)\n",
    "\n",
    "    #lets get the the chrome driver file stored in system so that we can acess the browser\n",
    "\n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    #now we have to search the job description and location to search the data\n",
    "    #there are two search boxes at the starting page\n",
    "    jobdescription = driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Scientist\")\n",
    "\n",
    "    #here i am searching for jobsin bangalore location\n",
    "    #and at last i am going to click on search button\n",
    "\n",
    "    button = driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "    button.click()\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    #now we have to click the checkboxes as per the problem statement the location filter is \"Delhi\" and salary is 3-6 lakh\n",
    "    #this is checkbox of location\n",
    "\n",
    "\n",
    "    try:\n",
    "        filter1 = driver.find_element_by_xpath(\"//span[@class='ellipsis fleft'][contains(text(),'Delhi / NCR')]\").click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"not accesable\")\n",
    "\n",
    "\n",
    "    #now lets create another filter for salary\n",
    "\n",
    "\n",
    "    try:\n",
    "        filter2 = driver.find_element_by_xpath(\"//span[contains(text(),'3-6 Lakhs')]\").click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"mot accessable\")\n",
    "\n",
    "\n",
    "    #actually .click() method used to click on the data, and we are done with filtering now lets start scraping results\n",
    "    #creating lists for storin data\n",
    "\n",
    "    job_title = []\n",
    "    job_location  = []\n",
    "    company_name = []\n",
    "    experience_list = []\n",
    "\n",
    "    #here lets start with the titles block here i am using xpath to find webelemunts to fetch job titles pf that page\n",
    "    #as we can see i have using slicling methd in the for loop to fetch only 10 values \n",
    "    # i have used exception handling so that we can execute the code easily with out any errors or exceptions\n",
    "    try:\n",
    "        titles = driver.find_elements_by_xpath(\"//a[@class = 'title fw500 ellipsis']\")\n",
    "        for title in titles[:10]:\n",
    "            job_title.append(title.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    # this is the blockfor comapny details, here  also i have used xpath for fetching\n",
    "\n",
    "    try:\n",
    "        company = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "        for com in company[:10]:\n",
    "            company_name.append(com.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    #this is the block for location data\n",
    "\n",
    "    try:\n",
    "        location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "        for loc in location[:10]:\n",
    "            job_location.append(loc.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    #this is the block for experience data \n",
    "\n",
    "    try:\n",
    "        experience = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "        for exp in experience[:10]:\n",
    "            experience_list.append(exp.text)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    #we have got all the required data lets create a datafrme so thet we can save the file in csv or json format\n",
    "\n",
    "    naukari_jobs3 = pd.DataFrame({})\n",
    "    \n",
    "    naukari_jobs3[\"Title\"] = job_title\n",
    "        \n",
    "    naukari_jobs3[\"Company\"] = company_name\n",
    "        \n",
    "    naukari_jobs3[\"Location\"] = job_location\n",
    "        \n",
    "    naukari_jobs3[\"Experience\"] = experience_list\n",
    "        \n",
    "    return naukari_jobs3\n",
    "\n",
    "#calling the function\n",
    "job3(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for first 10 job results for Data scientist\n",
    "Designation in Noida location. You have to scrape company_name, No. of days\n",
    "ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida”\n",
    "in “location” field.\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown\n",
    "page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Jobdays</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bechtel</td>\n",
       "      <td>17d</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>15d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lantern Digital Services</td>\n",
       "      <td>8d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>1d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sparkbpl</td>\n",
       "      <td>14d</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Priority Vendor</td>\n",
       "      <td>14d</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>23d</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mansha Solutions</td>\n",
       "      <td>10d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name Jobdays Rating\n",
       "0                   Bechtel     17d    4.0\n",
       "1        UnitedHealth Group     24h    3.6\n",
       "2                  Ericsson     15d    4.1\n",
       "3  Lantern Digital Services      8d    4.1\n",
       "4   Boston Consulting Group      1d    3.7\n",
       "5                  Sparkbpl     14d    3.8\n",
       "6           Priority Vendor     14d    3.1\n",
       "7            Biz2Credit Inc     23d    5.0\n",
       "8                  xtLytics     24h    3.8\n",
       "9          Mansha Solutions     10d    4.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def jobs3(url):\n",
    "    #first lets get the responce from the website weather we can scrap or not so lets start\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    #getting the driver details \n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    #Now lets we have to search for the role Data scientish and location noida and send_keys method sends the details to the textbox\n",
    "    job_title = driver.find_element_by_id(\"KeywordSearch\").send_keys(\"Data Scientist\")\n",
    "\n",
    "    #now lets search enter the location we want to enter\n",
    "    job_loc = driver.find_element_by_id(\"LocationSearch\").send_keys(\"Noida\")\n",
    "\n",
    "    #now click on the search button\n",
    "\n",
    "    button = driver.find_element_by_id(\"HeroSearchButton\").click() #click() methods used to click the button\n",
    "    \n",
    "    time.sleep(3)\n",
    "    #lets start scraping the data\n",
    "    #creating the lists to store results \n",
    "    company_name = []\n",
    "    jobposted_days = []\n",
    "    ratings = []\n",
    "    \n",
    "    #fetching comany name\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath('//div[@class=\"d-flex justify-content-between align-items-start\"]')\n",
    "        for i in name:\n",
    "            company_name.append(i.text)\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        company_name.append('-')\n",
    "        \n",
    "    #fetching jobposted days\n",
    "        \n",
    "    try:\n",
    "        day = driver.find_elements_by_xpath('//div[@class=\"d-flex align-items-end pl-std css-mi55ob\"]')\n",
    "        for i in day:\n",
    "            jobposted_days.append(i.text)\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        jobposted_days.append('-')\n",
    "        \n",
    "    \n",
    "    #fetching ratings\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_elements_by_xpath('//span[@class=\"css-19pjha7 e1cjmv6j1\"]')\n",
    "        for i in rating:\n",
    "            ratings.append(i.text)\n",
    "            \n",
    "    except NoSuchElementException:\n",
    "        ratings.appemd(\"-\")\n",
    "        \n",
    "    #lets create a dataframe so that we can store the data\n",
    "        \n",
    "    new_job=pd.DataFrame({})\n",
    "\n",
    "    new_job['Name']=company_name[:10]\n",
    "\n",
    "    new_job['Jobdays']=jobposted_days[:10]\n",
    "\n",
    "    new_job['Rating']=ratings[:10]\n",
    "\n",
    "    return new_job\n",
    "\n",
    "    #saving into csv file\n",
    "    \n",
    "    new_job.to_csv('new_job.csv') \n",
    "    \n",
    "\n",
    "#calling the function\n",
    "\n",
    "jobs3(\"https://www.glassdoor.co.in/Job/index.htm\")  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a python program to scrape the salary data for Data Scientist designation\n",
    "in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min\n",
    "salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page\n",
    "You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company\n",
    "name, Average salary and rating of the company.\n",
    "6.Store the data in a dataframe.\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comapany_name</th>\n",
       "      <th>Average_salary</th>\n",
       "      <th>Minimum_salary</th>\n",
       "      <th>Maximum_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>₹ 6,14,306/yr</td>\n",
       "      <td>₹577K</td>\n",
       "      <td>₹1,250K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹ 11,46,533/yr</td>\n",
       "      <td>₹586K</td>\n",
       "      <td>₹2,213K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>₹ 8,97,795/yr</td>\n",
       "      <td>₹355K</td>\n",
       "      <td>₹2,730K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹ 7,38,057/yr</td>\n",
       "      <td>₹450K</td>\n",
       "      <td>₹1,613K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹ 12,39,781/yr</td>\n",
       "      <td>₹1,069K</td>\n",
       "      <td>₹11,622K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹ 13,36,142/yr</td>\n",
       "      <td>₹502K</td>\n",
       "      <td>₹1,520K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹ 8,15,192/yr</td>\n",
       "      <td>₹202K</td>\n",
       "      <td>₹1,465K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ZS Associates</td>\n",
       "      <td>₹ 11,35,221/yr</td>\n",
       "      <td>₹575K</td>\n",
       "      <td>₹1,809K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>₹ 11,44,243/yr</td>\n",
       "      <td>₹1,014K</td>\n",
       "      <td>₹1,520K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Optum Global Solutions</td>\n",
       "      <td>₹ 14,13,288/yr</td>\n",
       "      <td>₹620K</td>\n",
       "      <td>₹2,149K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Comapany_name  Average_salary Minimum_salary Maximum_salary\n",
       "0  Tata Consultancy Services   ₹ 6,14,306/yr          ₹577K        ₹1,250K\n",
       "1                  Accenture  ₹ 11,46,533/yr          ₹586K        ₹2,213K\n",
       "2                        IBM   ₹ 8,97,795/yr          ₹355K        ₹2,730K\n",
       "3         Ericsson-Worldwide   ₹ 7,38,057/yr          ₹450K        ₹1,613K\n",
       "4                  Delhivery  ₹ 12,39,781/yr        ₹1,069K       ₹11,622K\n",
       "5         UnitedHealth Group  ₹ 13,36,142/yr          ₹502K        ₹1,520K\n",
       "6         Valiance Solutions   ₹ 8,15,192/yr          ₹202K        ₹1,465K\n",
       "7              ZS Associates  ₹ 11,35,221/yr          ₹575K        ₹1,809K\n",
       "8                EXL Service  ₹ 11,44,243/yr        ₹1,014K        ₹1,520K\n",
       "9     Optum Global Solutions  ₹ 14,13,288/yr          ₹620K        ₹2,149K"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def jobsalaries(url):\n",
    "    \n",
    "    #first lets get the responce from the website weather we can scrap or not so lets start\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    #getting the driver details \n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.maximize_window()\n",
    "\n",
    "    \n",
    "    #Now lets we have to search for the role Data scientish and location noida and send_keys method sends the details to the textbox\n",
    "    job_title = driver.find_element_by_id(\"KeywordSearch\").send_keys(\"Data Scientist\")\n",
    "\n",
    "    #now lets search enter the location we want to enter\n",
    "    job_loc = driver.find_element_by_id(\"LocationSearch\").send_keys(\"Noida\")\n",
    "\n",
    "    #now click on the search button\n",
    "\n",
    "    button = driver.find_element_by_id(\"HeroSearchButton\").click() #click() methods used to click the button\n",
    "    \n",
    "    #lets start with fetching company details\n",
    "    #lets create some lists \n",
    "    company_name = []\n",
    "    average_slalary = []\n",
    "    max_slalry = []\n",
    "    min_salary = []\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    #getting comany name, average salary and more detailas\n",
    "    \n",
    "    avg = driver.find_elements_by_xpath('//div[@class=\"col-2 d-none d-md-flex flex-row justify-content-end\"]')\n",
    "    for i in avg:\n",
    "        average_slalary.append(i.text.replace(\"\\n\", \"\"))\n",
    "        \n",
    "    name = driver.find_elements_by_xpath('//p[2]')\n",
    "    for i in name[2:]:\n",
    "        company_name.append(i.text)\n",
    "        \n",
    "    maxi = driver.find_elements_by_xpath('//div/span[2]')\n",
    "    for i in maxi[5:25]:\n",
    "        max_slalry.append(i.text.replace('[]', ''))\n",
    "        \n",
    "        \n",
    "    mini = driver.find_elements_by_xpath(\"//div/span[1]\")\n",
    "    for i in mini[64:83]:\n",
    "        min_salary.append(i.text.replace('[]', ''))\n",
    "        \n",
    "    #creating dataframe for storing results\n",
    "    \n",
    "    new_job1=pd.DataFrame({})\n",
    "         \n",
    "    new_job1['Comapany_name']=company_name[:10]\n",
    "\n",
    "    new_job1['Average_salary']=average_slalary[:10]\n",
    "\n",
    "    new_job1['Minimum_salary']=min_salary[:10]\n",
    "    \n",
    "    new_job1['Maximum_salary']=max_slalry[:10]\n",
    "    return new_job1\n",
    "    #saving in a csv file\n",
    "    new_job1.to_csv('new_job1.csv')\n",
    "\n",
    "#calling the function\n",
    "jobsalaries(\"https://www.glassdoor.co.in/Salaries/index.htm\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 : "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to\n",
    "scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and\n",
    "more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page\n",
    "you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of\n",
    "the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Prod_Desc</th>\n",
       "      <th>Prod_Price</th>\n",
       "      <th>Prod_Disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Badfella</td>\n",
       "      <td>Gradient, UV Protection Retro Square Sunglasse...</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IZAAN MART</td>\n",
       "      <td>Mirrored, UV Protection, Riding Glasses Rectan...</td>\n",
       "      <td>₹553</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹630</td>\n",
       "      <td>21% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection, Mirrored, Gradient Wayfarer Sun...</td>\n",
       "      <td>₹281</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹777</td>\n",
       "      <td>22% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹449</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Riding Glasses, UV Protection, Others Aviator ...</td>\n",
       "      <td>₹239</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>UV Protection Aviator Sunglasses (66)</td>\n",
       "      <td>₹938</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                          Prod_Desc  \\\n",
       "0         Badfella  Gradient, UV Protection Retro Square Sunglasse...   \n",
       "1       IZAAN MART  Mirrored, UV Protection, Riding Glasses Rectan...   \n",
       "2         Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "3         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "4   ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)   \n",
       "..             ...                                                ...   \n",
       "95          GANSTA  UV Protection, Mirrored, Gradient Wayfarer Sun...   \n",
       "96        Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "97  ROZZETTA CRAFT  UV Protection, Gradient Round Sunglasses (Free...   \n",
       "98    Singco India  Riding Glasses, UV Protection, Others Aviator ...   \n",
       "99          AISLIN              UV Protection Aviator Sunglasses (66)   \n",
       "\n",
       "   Prod_Price Prod_Disc  \n",
       "0        ₹349   65% off  \n",
       "1        ₹553   73% off  \n",
       "2        ₹630   21% off  \n",
       "3        ₹758   15% off  \n",
       "4        ₹499   77% off  \n",
       "..        ...       ...  \n",
       "95       ₹281   85% off  \n",
       "96       ₹777   22% off  \n",
       "97       ₹449   83% off  \n",
       "98       ₹239   84% off  \n",
       "99       ₹938   74% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def sunglasses(url):\n",
    "    \n",
    "    #getting the driver in the system\n",
    "\n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    #canceling the login popup\n",
    "\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #entering the item that need to search and the click on the button so that we will be navigate to that page\n",
    "\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sunglasses')\n",
    "\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    \n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #creating emptylists for storing data for creating data frame\n",
    "\n",
    "    brand_name=[]\n",
    "\n",
    "    prod_desc=[]\n",
    "\n",
    "    prod_price=[]\n",
    "\n",
    "    prod_disc=[]\n",
    "    \n",
    "    #we have to grab 100 sunglasses so we are going to creating aloop to visit each page  and and collect the data\n",
    "    # data like brand proce and discount, and later sore them in a dataframe\n",
    "    \n",
    "    j=0\n",
    "\n",
    "    while j<3:\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "        for brand in brands:\n",
    "\n",
    "            brand_name.append(brand.text)\n",
    "\n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "\n",
    "        for description in descriptions:\n",
    "\n",
    "            prod_desc.append(description.text)\n",
    "\n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "        for price in prices:\n",
    "\n",
    "            prod_price.append(price.text)\n",
    "\n",
    "        discounts=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "        for discount in discounts:\n",
    "\n",
    "            prod_disc.append(discount.text)\n",
    "\n",
    "        if j==0:\n",
    "\n",
    "            driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "\n",
    "        elif j==1:\n",
    "\n",
    "            driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]').click()\n",
    "\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "\n",
    "        j+=1\n",
    "        \n",
    "    #creating datafram to store the data\n",
    "\n",
    "    sneakers=pd.DataFrame({})\n",
    "\n",
    "    sneakers['Brand']=brand_name[:100]\n",
    "\n",
    "    sneakers['Prod_Desc']=prod_desc[:100]\n",
    "\n",
    "    sneakers['Prod_Price']=prod_price[:100]\n",
    "\n",
    "    sneakers['Prod_Disc']=prod_disc[:100]\n",
    "\n",
    "    return sneakers\n",
    "\n",
    "    sneakers.to_csv('sneakers.csv')\n",
    "    \n",
    "#saving the file in csv format\n",
    "sunglasses(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to\n",
    "go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-\n",
    "earpods-power-\n",
    "adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage.\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47934ec17556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m#CALLING THE FUNCTION WITH URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mreviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-47934ec17556>\u001b[0m in \u001b[0;36mreviews\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#getting the driver details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/santosh/Downloads/chromedriver_linux64/chromedriver'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#getting the url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mremote_server_addr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     keep_alive=keep_alive),\n\u001b[0;32m---> 81\u001b[0;31m                 desired_capabilities=desired_capabilities)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    155\u001b[0m             warnings.warn(\"Please use FirefoxOptions to set browser profile\",\n\u001b[1;32m    156\u001b[0m                           DeprecationWarning, stacklevel=2)\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrowser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwitchTo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mobile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMobile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[1;32m    250\u001b[0m         parameters = {\"capabilities\": w3c_caps,\n\u001b[1;32m    251\u001b[0m                       \"desiredCapabilities\": capabilities}\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'sessionId'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\n"
     ]
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def reviews(url):\n",
    "    \n",
    "    #first lets get the responce from the website weather we can scrap or not so lets start\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    #getting the driver details \n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    \n",
    "    #getting the url\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    driver.maximize_window()\n",
    "    \n",
    "    driver.refresh()\n",
    "    \n",
    "    #clicking on all reviews button so that we can grab the data\n",
    "    \n",
    "    click = driver.find_element_by_xpath('//div[@class=\"_3UAT2v _16PBlm\"]').click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    rating_list = []\n",
    "    \n",
    "    review_list = []\n",
    "    \n",
    "    summary_list = []\n",
    "    \n",
    "    #here we have to collect 100 reviews so we have to loop throuh all pages so that we can collect the review\n",
    "    #basically review data containd review, rating, and description\n",
    "    #here is the loop where iam going to traverse each page and collects the data\n",
    "    j = 0\n",
    "    \n",
    "    while j < 12:\n",
    "        \n",
    "        driver.refresh() #it helps to avoid stale exception in the data\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            rating = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        \n",
    "            for ii in rating:\n",
    "            \n",
    "                rating_list.append(ii.text)\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            review = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "            \n",
    "            for jj in review:\n",
    "                \n",
    "                review_list.append(jj.text)\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            summary = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div\")\n",
    "            \n",
    "            for kk in summary:\n",
    "                \n",
    "                summary_list.append(kk.text)\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        if j == 0:\n",
    "            \n",
    "            driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "            \n",
    "        elif j == 1:\n",
    "            \n",
    "            driver.find_element_by_xpath('//a[@class=\"ge-49M\"]').click()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        j+=1\n",
    "        \n",
    "    #creating dataframe for storing data\n",
    "    \n",
    "    filpkart_reviews = pd.DataFrame({})\n",
    "    \n",
    "    filpkart_reviews['Rating']=rating_list[:100]\n",
    "\n",
    "    filpkart_reviews['Review']=review_list[:100]\n",
    "\n",
    "    filpkart_reviews[\"summary\"]=summary_list[:100]\n",
    "\n",
    "    return filpkart_reviews\n",
    "\n",
    "    filpkart_reviews.to_csv('filpkart_reviews.csv')\n",
    "    \n",
    "            \n",
    "#CALLING THE FUNCTION WITH URL        \n",
    "reviews(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and\n",
    "search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "Also note that all the steps required during scraping should be done through code\n",
    "only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Prod_Desc</th>\n",
       "      <th>Prod_Price</th>\n",
       "      <th>Prod_Disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹379</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹474</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shoes Bank</td>\n",
       "      <td>White Sneaker For Men's/Boy's Sneakers For Men</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stefano Rads</td>\n",
       "      <td>Classy Sneakers For Men</td>\n",
       "      <td>₹229</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bluemaker</td>\n",
       "      <td>casual for men (blue 06) Sneakers For Men</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Englewood</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>33% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>X-Ray 2 Square PACK Sneakers For Men</td>\n",
       "      <td>₹2,939</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>HOCKWOOD</td>\n",
       "      <td>Street Smart Sneakers For Men</td>\n",
       "      <td>₹664</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>bacca bucci</td>\n",
       "      <td>Men's Ultraforce Mid-top Athletic-Inspired Ret...</td>\n",
       "      <td>₹1,051</td>\n",
       "      <td>59% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>LoveHush</td>\n",
       "      <td>Men's Denim Casual Sneakers Jeans Shoes Sneake...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                                          Prod_Desc  \\\n",
       "0   Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "1   Robbie jones                                   Sneakers For Men   \n",
       "2     Shoes Bank     White Sneaker For Men's/Boy's Sneakers For Men   \n",
       "3   Stefano Rads                            Classy Sneakers For Men   \n",
       "4      bluemaker          casual for men (blue 06) Sneakers For Men   \n",
       "..           ...                                                ...   \n",
       "95     Englewood                                   Sneakers For Men   \n",
       "96          PUMA               X-Ray 2 Square PACK Sneakers For Men   \n",
       "97      HOCKWOOD                      Street Smart Sneakers For Men   \n",
       "98   bacca bucci  Men's Ultraforce Mid-top Athletic-Inspired Ret...   \n",
       "99      LoveHush  Men's Denim Casual Sneakers Jeans Shoes Sneake...   \n",
       "\n",
       "   Prod_Price Prod_Disc  \n",
       "0        ₹379   62% off  \n",
       "1        ₹474   52% off  \n",
       "2        ₹349   65% off  \n",
       "3        ₹229   67% off  \n",
       "4        ₹449   55% off  \n",
       "..        ...       ...  \n",
       "95       ₹499   33% off  \n",
       "96     ₹2,939   57% off  \n",
       "97       ₹664   60% off  \n",
       "98     ₹1,051   59% off  \n",
       "99       ₹399   25% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def sneakers(url):\n",
    "    \n",
    "    #getting chrome driver \n",
    "\n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    \n",
    "    #finding url\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    #canceling the login popup\n",
    "\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #we have to search sneakes so we have to enter sneakers in the search buttom and the press click\n",
    "\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sneakers')\n",
    "\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #creating the empty list so that w have to gram the data and store in it\n",
    "\n",
    "    brand_name=[]\n",
    "\n",
    "    prod_desc=[]\n",
    "\n",
    "    prod_price=[]\n",
    "\n",
    "    prod_disc=[]\n",
    "    \n",
    "    #here we have to collect 100 sneakers so we have to loop throuh all pages so that we can collect the review\n",
    "    #basically review data containd review, rating, and description\n",
    "    #here is the loop where iam going to traverse each page and collects the data\n",
    "\n",
    "    j=0\n",
    "\n",
    "    while j<3:\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "        for brand in brands:\n",
    "\n",
    "            brand_name.append(brand.text)\n",
    "\n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "\n",
    "        for description in descriptions:\n",
    "\n",
    "            prod_desc.append(description.text)\n",
    "\n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "\n",
    "        for price in prices:\n",
    "\n",
    "            prod_price.append(price.text)\n",
    "\n",
    "        discounts=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "\n",
    "        for discount in discounts:\n",
    "\n",
    "            prod_disc.append(discount.text)\n",
    "\n",
    "        if j==0:\n",
    "\n",
    "            driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "\n",
    "        elif j==1:\n",
    "\n",
    "            driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]').click()\n",
    "\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "\n",
    "        j+=1\n",
    "        \n",
    "    #creating a dataframe so we can stor the data\n",
    "\n",
    "    sneakers=pd.DataFrame({})\n",
    "\n",
    "    sneakers['Brand']=brand_name[:100]\n",
    "\n",
    "    sneakers['Prod_Desc']=prod_desc[:100]\n",
    "\n",
    "    sneakers['Prod_Price']=prod_price[:100]\n",
    "\n",
    "    sneakers['Prod_Disc']=prod_disc[:100]\n",
    "\n",
    "    return sneakers\n",
    "\n",
    "    sneakers.to_csv('sneakers.csv')\n",
    "    \n",
    "#calling the functions     \n",
    "sneakers(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in\n",
    "the below image\n",
    "nd then scrape First 100 shoes data you get. The data should include “Brand” of\n",
    "the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be\n",
    "done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Unisex Mercedes Running Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men KD13 EP Basketball Shoes</td>\n",
       "      <td>Rs. 12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men React Infinity Running</td>\n",
       "      <td>Rs. 11621Rs. 15495(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Women PEGASUS 37 Running Shoes</td>\n",
       "      <td>Rs. 7496Rs. 9995(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men JORDAN DELTA Basketball</td>\n",
       "      <td>Rs. 12495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Derbys</td>\n",
       "      <td>Rs. 11490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men REACT MILER 2 Running</td>\n",
       "      <td>Rs. 9770Rs. 11495(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Charged RC Sportstyle Sneakers</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 11242Rs. 14990(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ruosh</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 6990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand                               Name  \\\n",
       "0   PUMA Motorsport      Unisex Mercedes Running Shoes   \n",
       "1              Nike       Men KD13 EP Basketball Shoes   \n",
       "2              Nike         Men React Infinity Running   \n",
       "3              Nike     Women PEGASUS 37 Running Shoes   \n",
       "4              Nike        Men JORDAN DELTA Basketball   \n",
       "..              ...                                ...   \n",
       "95             Geox          Men Leather Formal Derbys   \n",
       "96             Nike          Men REACT MILER 2 Running   \n",
       "97     UNDER ARMOUR     Charged RC Sportstyle Sneakers   \n",
       "98             Geox        Men Leather Formal Slip-Ons   \n",
       "99            Ruosh  Men Solid Leather Formal Slip-Ons   \n",
       "\n",
       "                          Price  \n",
       "0                      Rs. 7999  \n",
       "1                     Rs. 12995  \n",
       "2   Rs. 11621Rs. 15495(25% OFF)  \n",
       "3     Rs. 7496Rs. 9995(25% OFF)  \n",
       "4                     Rs. 12495  \n",
       "..                          ...  \n",
       "95                    Rs. 11490  \n",
       "96   Rs. 9770Rs. 11495(15% OFF)  \n",
       "97                     Rs. 8999  \n",
       "98  Rs. 11242Rs. 14990(25% OFF)  \n",
       "99                     Rs. 6990  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def shoes(url):\n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    \n",
    "    #getting the url\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”,\n",
    "    \n",
    "    filter1 = driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]/label\").click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    filter2 = driver.find_element_by_xpath(\"//li[@class][1]/label\").click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    #creating an empty list for storing results\n",
    "    \n",
    "    product_name = []\n",
    "    \n",
    "    product_desc = []\n",
    "    \n",
    "    product_price = []\n",
    "    \n",
    "    #here we have to collect 100 sneakers so we have to loop throuh all pages so that we can collect the review\n",
    "    #basically review data containd review, rating, and description and brand etc\n",
    "    #here is the loop where iam going to traverse each page and collects the data\n",
    "    \n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    while j < 3:\n",
    "        \n",
    "        time.sleep(3)\n",
    "        \n",
    "        brands=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "\n",
    "        for brand in brands:\n",
    "\n",
    "            product_name.append(brand.text)\n",
    "            \n",
    "        \n",
    "        descriptions=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "\n",
    "        for description in descriptions:\n",
    "\n",
    "            product_desc.append(description.text)\n",
    "            \n",
    "        \n",
    "        \n",
    "        price = driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "        \n",
    "        for pr in price:\n",
    "            \n",
    "            product_price.append(pr.text)\n",
    "        \n",
    "        if j == 0:\n",
    "            \n",
    "            driver.find_element_by_xpath(\"//li[@class='pagination-next']\").click()\n",
    "            \n",
    "        elif j ==1:\n",
    "            \n",
    "            driver.find_element_by_xpath(\"//li[@class='pagination-active']\").click()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "        j+=1\n",
    "    \n",
    "    #creating the datafram to store the results\n",
    "    \n",
    "    shoes = pd.DataFrame({})\n",
    "    \n",
    "    shoes[\"Brand\"] = product_name[:100]\n",
    "    \n",
    "    shoes[\"Name\"] = product_desc[:100]\n",
    "    \n",
    "    shoes[\"Price\"] = product_price[:100]\n",
    "    \n",
    "    return shoes\n",
    "\n",
    "    shoes.to_csv(\"shoes.csv\") \n",
    "    \n",
    "#calling the function\n",
    "shoes(\"https://www.myntra.com/shoes?\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes\n",
    "for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo IdeaPad Flex 5 11th Gen Intel Core i3 1...</td>\n",
       "      <td>₹ 50,792.00</td>\n",
       "      <td>51 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...</td>\n",
       "      <td>₹ 68,990.00</td>\n",
       "      <td>115 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 15 Entry Level 15.6-inch (39.62 cms) HD Lap...</td>\n",
       "      <td>₹ 23,990.00</td>\n",
       "      <td>596 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVITA Essential NE14A2INC433-MB 14\" (35.56cms)...</td>\n",
       "      <td>₹ 18,990.00</td>\n",
       "      <td>573 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS VivoBook 14 (2020) Intel Quad Core Pentiu...</td>\n",
       "      <td>₹ 24,990.00</td>\n",
       "      <td>68 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenovo ThinkPad E14 Intel Core i5 10th Gen 14-...</td>\n",
       "      <td>₹ 65,047.00</td>\n",
       "      <td>349 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Acer Aspire 3 Intel Core i5-10th Gen 15.6\" (39...</td>\n",
       "      <td>₹ 42,900.00</td>\n",
       "      <td>223 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DELL Inspiron 3493 14\" (35.56cms) FHD Thin &amp; L...</td>\n",
       "      <td>₹ 27,990.00</td>\n",
       "      <td>524 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo IdeaPad Slim 3 Intel Celeron N4020 15.6...</td>\n",
       "      <td>₹ 27,490.00</td>\n",
       "      <td>170 ratings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo Ideapad S145 AMD Ryzen 3 3200U 15.6 inc...</td>\n",
       "      <td>₹ 26,999.00</td>\n",
       "      <td>114 ratings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Brand         Name        Price\n",
       "0  Lenovo IdeaPad Flex 5 11th Gen Intel Core i3 1...  ₹ 50,792.00   51 ratings\n",
       "1  HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...  ₹ 68,990.00  115 ratings\n",
       "2  HP 15 Entry Level 15.6-inch (39.62 cms) HD Lap...  ₹ 23,990.00  596 ratings\n",
       "3  AVITA Essential NE14A2INC433-MB 14\" (35.56cms)...  ₹ 18,990.00  573 ratings\n",
       "4  ASUS VivoBook 14 (2020) Intel Quad Core Pentiu...  ₹ 24,990.00   68 ratings\n",
       "5  Lenovo ThinkPad E14 Intel Core i5 10th Gen 14-...  ₹ 65,047.00  349 ratings\n",
       "6  Acer Aspire 3 Intel Core i5-10th Gen 15.6\" (39...  ₹ 42,900.00  223 ratings\n",
       "7  DELL Inspiron 3493 14\" (35.56cms) FHD Thin & L...  ₹ 27,990.00  524 ratings\n",
       "8  Lenovo IdeaPad Slim 3 Intel Celeron N4020 15.6...  ₹ 27,490.00  170 ratings\n",
       "9  Lenovo Ideapad S145 AMD Ryzen 3 3200U 15.6 inc...  ₹ 26,999.00  114 ratings"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import chrome\n",
    "from selenium.common.exceptions import *\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def laptop(url):\n",
    "    \n",
    "    #getting the driver details \n",
    "    driver = webdriver.Chrome('/home/santosh/Downloads/chromedriver_linux64/chromedriver')\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    driver.maximize_window()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        search = driver.find_element_by_id('twotabsearchtextbox').send_keys(\"Laptop\")\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"not accesable\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "        searchbtn = driver.find_element_by_id('nav-search-submit-button').click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"not accesable\")\n",
    "    \n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #setting the filters\n",
    "    try:\n",
    "        filter1 = driver.find_element_by_xpath(\"//span[contains(text(),'Intel Core i7')]\").click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    urls = []\n",
    "    url = driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for i in url:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "        \n",
    "        \n",
    "    title_list = []\n",
    "    price_list = []\n",
    "    rating_list = []\n",
    "    \n",
    "    #feting the details like title, price and rating\n",
    "    \n",
    "    for i in urls[:20]:\n",
    "        driver.get(i)\n",
    "        time.sleep(4)\n",
    "        try:\n",
    "            title = driver.find_elements_by_id(\"productTitle\")\n",
    "            for i in title:\n",
    "                title_list.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            title_list.append('-')\n",
    "    \n",
    "        try:\n",
    "            price = driver.find_elements_by_xpath(\"//span[@id='priceblock_ourprice']\")\n",
    "            for i in price:\n",
    "                price_list.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            price_list.append('-')\n",
    "        \n",
    "        try:\n",
    "            rating = driver.find_elements_by_xpath(\"//div[5]/div/span[3]/a/span\")\n",
    "            for i in rating:\n",
    "                rating_list.append(i.text)\n",
    "        except NoSuchElementException:\n",
    "            rating_list.append('-')\n",
    "            \n",
    "    #creating a dataframe        \n",
    "            \n",
    "    laptops = pd.DataFrame({})\n",
    "    \n",
    "    laptops[\"Brand\"] = title_list[:10]\n",
    "    \n",
    "    laptops[\"Name\"] = price_list[:10]\n",
    "    \n",
    "    laptops[\"Price\"] = rating_list[:10]\n",
    "    \n",
    "    return laptops\n",
    "\n",
    "    laptops.to_csv(\"laptops.csv\")  \n",
    " \n",
    "\n",
    "\n",
    " #calling the function\n",
    "\n",
    "laptop(\"https://www.amazon.in\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python38364bitbasecondaf5ae4ab580394e4693ea5507f08b0bfd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
